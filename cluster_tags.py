import pandas as pd
import re
from thefuzz import fuzz
from collections import defaultdict

# ==============================================================================
# æ­¥éª¤ 1: ä¸“ä¸ºâ€œæ ‡ç­¾â€è®¾è®¡çš„æ¸…æ´—é¢„å¤„ç†å‡½æ•°
# ==============================================================================
def robust_clean_tag_name(tag):
    """
    ä¸€ä¸ªä¸“ç”¨äºæ ‡ç­¾èšç±»é¢„å¤„ç†çš„æ¸…æ´—å‡½æ•°ã€‚
    å®ƒä¼šç§»é™¤æ¦œå•ã€å¹´ä»½ã€åœ°ç†ä½ç½®ç­‰é€šç”¨å™ªéŸ³è¯ã€‚
    """
    if not isinstance(tag, str): return ""
    
    # ç§»é™¤è¿å­—ç¬¦å’Œæ‹¬å·åŠå…¶å†…å®¹
    clean_tag = tag.split('-', 1)[0].strip()
    clean_tag = re.sub(r'[ï¼ˆ\(][^ï¼‰)]*[ï¼‰\)]', '', clean_tag)
    
    # è½¬ä¸ºå°å†™
    clean_tag = clean_tag.lower()

    # å®šä¹‰éœ€è¦ç§»é™¤çš„å™ªéŸ³è¯ (é’ˆå¯¹æ¦œå•/æ ‡ç­¾å)
    stop_words = [
        'ä¸­å›½', 'ä¼ä¸š', 'å…¬å¸', 'é›†å›¢', 'æ§è‚¡', 'è‚¡ä»½', 'ä¸Šå¸‚',
        'æ¦œå•', 'åå•', 'æ’å', 'å¼º', 'å®¶', 'æ¦œ',
        'å¹´åº¦', 'å‘å¸ƒ', 'æœ€æ–°', 'å¹´',
        'the', 'of', 'and', 'in' # ç®€å•çš„è‹±æ–‡åœç”¨è¯
    ]
    
    # ç§»é™¤æ‰€æœ‰æ•°å­— (é€šå¸¸æ˜¯å¹´ä»½æˆ–æ’åæ•°å­—ï¼Œä¼šå¹²æ‰°ç›¸ä¼¼åº¦åˆ¤æ–­)
    clean_tag = re.sub(r'\d+', '', clean_tag)

    # ä½¿ç”¨å•è¯è¾¹ç•Œ\bè¿›è¡Œç²¾ç¡®ç§»é™¤
    for word in stop_words:
        clean_tag = re.sub(r'\b' + re.escape(word) + r'\b', '', clean_tag)
        
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    clean_tag = re.sub(r'\s+', ' ', clean_tag).strip()
    
    return clean_tag

# ==============================================================================
# æ­¥éª¤ 2: èšç±»å‡½æ•° (ä¿æŒé€šç”¨æ€§)
# ==============================================================================
def cluster_items(items, preprocess_func, threshold=85):
    """
    é€šç”¨çš„èšç±»å‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ã€‚

    Args:
        items (list): å¾…èšç±»çš„å­—ç¬¦ä¸²åˆ—è¡¨ (è¿™é‡Œæ˜¯æ ‡ç­¾å)ã€‚
        preprocess_func (function): ç”¨äºé¢„å¤„ç†æ¯ä¸ªå­—ç¬¦ä¸²çš„å‡½æ•°ã€‚
        threshold (int): ç›¸ä¼¼åº¦é˜ˆå€¼ã€‚

    Returns:
        list: èšç±»ç»“æœï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«ç›¸ä¼¼é¡¹çš„é›†åˆ(set)ã€‚
    """
    print(f"å¼€å§‹å¯¹ {len(items)} ä¸ªå”¯ä¸€é¡¹è¿›è¡Œèšç±»ï¼Œé˜ˆå€¼={threshold}%...")
    clusters = []
    
    preprocessed_map = {item: preprocess_func(item) for item in items}
    
    sorted_items = sorted(items, key=lambda x: len(preprocessed_map[x]), reverse=True)

    for item in sorted_items:
        processed_item = preprocessed_map[item]
        if not processed_item: continue

        found_cluster = False
        for cluster in clusters:
            representative = next(iter(cluster))
            processed_representative = preprocessed_map[representative]
            
            score = fuzz.token_set_ratio(processed_item, processed_representative)
            
            if score > threshold:
                cluster.add(item)
                found_cluster = True
                break
        
        if not found_cluster:
            clusters.append({item})
            
    print(f"èšç±»å®Œæˆï¼Œå…±å½¢æˆ {len(clusters)} ä¸ªç°‡ã€‚")
    return clusters

# ==============================================================================
# æ­¥éª¤ 3: ä¸»æ‰§è¡Œæµç¨‹ (å·²ä¿®æ”¹ä¸ºå¤„ç†æ ‡ç­¾)
# ==============================================================================
def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    input_csv = 'æ ‡ç­¾å_å…¬å¸å_final.csv'
    output_csv = 'tag_clusters_for_review.csv'
    
    # --- 1. åŠ è½½å¹¶æå–æ‰€æœ‰å”¯ä¸€çš„æ ‡ç­¾å ---
    try:
        df = pd.read_csv(input_csv, encoding='utf-8-sig', usecols=['æ ‡ç­¾å'])
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{input_csv}' ä¸å­˜åœ¨ã€‚")
        return
    except ValueError:
        print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ '{input_csv}' ä¸­æœªæ‰¾åˆ°'æ ‡ç­¾å'åˆ—ã€‚")
        return

    # å°†"æ ‡ç­¾å"åˆ—ä¸­çš„é€—å·åˆ†éš”å­—ç¬¦ä¸²æ‹†åˆ†æˆåˆ—è¡¨
    df['æ ‡ç­¾å'] = df['æ ‡ç­¾å'].astype(str).str.split(',')
    # ä½¿ç”¨ explode å°†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ æ‰©å±•æˆæ–°è¡Œ
    exploded_df = df.explode('æ ‡ç­¾å')
    # æ¸…ç†å‰åç©ºæ ¼å¹¶åˆ é™¤ç©ºè¡Œ
    exploded_df['æ ‡ç­¾å'] = exploded_df['æ ‡ç­¾å'].str.strip()
    exploded_df.dropna(subset=['æ ‡ç­¾å'], inplace=True)
    exploded_df = exploded_df[exploded_df['æ ‡ç­¾å'] != '']
    
    # è·å–æ‰€æœ‰å”¯ä¸€çš„æ ‡ç­¾å
    unique_tags = exploded_df['æ ‡ç­¾å'].unique().tolist()
    
    # --- 2. å¯¹å”¯ä¸€æ ‡ç­¾è¿›è¡Œèšç±» ---
    # å¯¹äºæ ‡ç­¾ï¼Œé˜ˆå€¼å¯ä»¥é€‚å½“æ”¾å®½ï¼Œä¾‹å¦‚85
    tag_clusters = cluster_items(unique_tags, 
                                 preprocess_func=robust_clean_tag_name, 
                                 threshold=70)
    
    # --- 3. å‡†å¤‡å¹¶è¾“å‡ºä¾›äººå·¥å®¡æ ¸çš„ç»“æœ ---
    review_data = []
    for i, cluster in enumerate(tag_clusters):
        # åªè¦ç°‡é‡Œæœ‰å†…å®¹å°±è¾“å‡ºï¼Œå› ä¸ºå³ä½¿æ˜¯å•ä¸ªæ ‡ç­¾ä¹Ÿå¯èƒ½éœ€è¦æ ‡å‡†åŒ–
        if cluster:
            # é€‰æ‹©ç°‡ä¸­æœ€é•¿çš„åå­—ä½œä¸ºå»ºè®®çš„æ ‡å‡†å
            suggested_standard_name = max(cluster, key=len)
            
            # å…¶ä»–åå­—ä½œä¸ºåˆ«å
            aliases = sorted(list(cluster - {suggested_standard_name}))
            
            review_data.append({
                'Cluster_ID': i + 1,
                'Standard_Name_Suggestion': suggested_standard_name,
                'Aliases_Found': ' | '.join(aliases),
                'Cluster_Size': len(cluster)
            })
        
    review_df = pd.DataFrame(review_data)
    # æŒ‰ç°‡çš„å¤§å°é™åºæ’åˆ—ï¼Œä¼˜å…ˆå®¡æ ¸æœ€å¯èƒ½é‡å¤çš„ç»„
    review_df = review_df.sort_values(by='Cluster_Size', ascending=False)
    
    review_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼å…±å‘ç° {len(review_df)} ä¸ªæ ‡ç­¾ç°‡ã€‚")
    print(f"è¯·æ‰“å¼€ '{output_csv}' æ–‡ä»¶è¿›è¡Œäººå·¥å®¡æ ¸ã€‚")

if __name__ == '__main__':
    main()