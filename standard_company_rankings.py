import os
import json
import csv
import re
from collections import defaultdict

# ==============================================================================
# æ­¥éª¤ 1: å®šä¹‰æ¸…æ´—å‡½æ•°å’Œåœç”¨è¯åº“ 
# ==============================================================================

SMART_STOP_WORDS = {
    # ==========================================================================
    # ç»„ç»‡åç¼€ (å·²å¤§å¹…å¢å¼ºè‹±æ–‡éƒ¨åˆ†)
    # ==========================================================================
    "SUFFIXES": [
        # --- ä¸­æ–‡éƒ¨åˆ† (ä¿æŒä¸å˜) ---
        'è‚¡ä»½æœ‰é™å…¬å¸', 'æœ‰é™è´£ä»»å…¬å¸', 'è‚¡ä»½åˆä½œå…¬å¸', 'æœ‰é™å…¬å¸', 'åˆä¼™ä¼ä¸š',
        'æ€»å…¬å¸', 'åˆ†å…¬å¸', 'å…¬å¸', 'é›†å›¢', 'åŸºé‡‘ä¼š', 'åŠäº‹å¤„', 'åˆä½œç¤¾',
        'ç ”ç©¶æ‰€', 'è®¾è®¡é™¢', 'ä¸­å¿ƒ', 'åä¼š', 'å­¦ä¼š', 'å§”å‘˜ä¼š', 'åŠå…¬å®¤',
        'ç¼–è¾‘éƒ¨', 'å‡ºç‰ˆç¤¾', 'æ€»å‚', 'åˆ†å‚',
        
        # --- è‹±æ–‡å¸¸è§å½¢å¼ (æŒ‰é•¿åº¦é™åºæ’åˆ—) ---
        'corporation', 'incorporated', 'co.,ltd.', 'co., ltd.', 'co.,ltd',
        'company', 'limited', 'co.ltd', 'co ltd', 'group', 'corp.', 'corp',
        'inc.', 'inc', 'llc', 'ltd.', 'ltd', 'co.', 'co',
        # --- å…¶ä»–å›½é™…å½¢å¼ ---
        'gmbh', 's.a.', 's.l.', 'a.g.', 'pty ltd'
    ],

    # ==========================================================================
    # åœ°ç†ä½ç½® (ä¿æŒä¸å˜)
    # ==========================================================================
     "LOCATIONS": [
        # ============================ ä¸­æ–‡éƒ¨åˆ† (ä¿ç•™) ============================
        # --- ç›´è¾–å¸‚ ---
        'åŒ—äº¬å¸‚', 'åŒ—äº¬', 'å¤©æ´¥å¸‚', 'å¤©æ´¥', 'ä¸Šæµ·å¸‚', 'ä¸Šæµ·', 'é‡åº†å¸‚', 'é‡åº†',
        # --- ååŒ—åœ°åŒº ---
        'æ²³åŒ—çœ', 'æ²³åŒ—', 'çŸ³å®¶åº„å¸‚', 'çŸ³å®¶åº„', 'å”å±±å¸‚', 'å”å±±',
        'å±±è¥¿çœ', 'å±±è¥¿', 'å¤ªåŸå¸‚', 'å¤ªåŸ', 'å¤§åŒå¸‚', 'å¤§åŒ',
        'å†…è’™å¤è‡ªæ²»åŒº', 'å†…è’™å¤', 'å‘¼å’Œæµ©ç‰¹å¸‚', 'å‘¼å’Œæµ©ç‰¹', 'åŒ…å¤´å¸‚', 'åŒ…å¤´',
        # --- ä¸œåŒ—åœ°åŒº ---
        'è¾½å®çœ', 'è¾½å®', 'æ²ˆé˜³å¸‚', 'æ²ˆé˜³', 'å¤§è¿å¸‚', 'å¤§è¿',
        'å‰æ—çœ', 'å‰æ—', 'é•¿æ˜¥å¸‚', 'é•¿æ˜¥',
        'é»‘é¾™æ±Ÿçœ', 'é»‘é¾™æ±Ÿ', 'å“ˆå°”æ»¨å¸‚', 'å“ˆå°”æ»¨', 'å¤§åº†å¸‚', 'å¤§åº†',
        # --- åä¸œåœ°åŒº ---
        'æ±Ÿè‹çœ', 'æ±Ÿè‹', 'å—äº¬å¸‚', 'å—äº¬', 'è‹å·å¸‚', 'è‹å·', 'æ— é”¡å¸‚', 'æ— é”¡',
        'æµ™æ±Ÿçœ', 'æµ™æ±Ÿ', 'æ­å·å¸‚', 'æ­å·', 'å®æ³¢å¸‚', 'å®æ³¢', 'æ¸©å·å¸‚', 'æ¸©å·',
        'å®‰å¾½çœ', 'å®‰å¾½', 'åˆè‚¥å¸‚', 'åˆè‚¥', 'ç¦å»ºçœ',
        'ç¦å»º', 'ç¦å·å¸‚', 'ç¦å·', 'å¦é—¨å¸‚', 'å¦é—¨', 'æ³‰å·å¸‚', 'æ³‰å·',
        'æ±Ÿè¥¿çœ', 'æ±Ÿè¥¿', 'å—æ˜Œå¸‚', 'å—æ˜Œ',
        'å±±ä¸œçœ', 'å±±ä¸œ', 'æµå—å¸‚', 'æµå—', 'é’å²›å¸‚', 'é’å²›', 'çƒŸå°å¸‚', 'çƒŸå°',
        # --- åä¸­åœ°åŒº ---
        'æ²³å—çœ', 'æ²³å—', 'éƒ‘å·å¸‚', 'éƒ‘å·', 'æ´›é˜³å¸‚', 'æ´›é˜³',
        'æ¹–åŒ—çœ', 'æ¹–åŒ—', 'æ­¦æ±‰å¸‚', 'æ­¦æ±‰', 'å®œæ˜Œå¸‚', 'å®œæ˜Œ',
        'æ¹–å—çœ', 'æ¹–å—', 'é•¿æ²™å¸‚', 'é•¿æ²™',
        # --- åå—åœ°åŒº ---
        'å¹¿ä¸œçœ', 'å¹¿ä¸œ', 'å¹¿å·å¸‚', 'å¹¿å·', 'æ·±åœ³å¸‚', 'æ·±åœ³', 'ä¸œèå¸‚', 'ä¸œè', 'ä½›å±±å¸‚', 'ä½›å±±', 'ç æµ·å¸‚', 'ç æµ·',
        'å¹¿è¥¿å£®æ—è‡ªæ²»åŒº', 'å¹¿è¥¿', 'å—å®å¸‚', 'å—å®', 'æ¡‚æ—å¸‚', 'æ¡‚æ—',
        'æµ·å—çœ', 'æµ·å—', 'æµ·å£å¸‚', 'æµ·å£', 'ä¸‰äºšå¸‚', 'ä¸‰äºš',
        # --- è¥¿å—åœ°åŒº ---
        'å››å·çœ', 'å››å·', 'æˆéƒ½å¸‚', 'æˆéƒ½', 'ç»µé˜³å¸‚', 'ç»µé˜³',
        'è´µå·çœ', 'è´µå·', 'è´µé˜³å¸‚', 'è´µé˜³',
        'äº‘å—çœ', 'äº‘å—', 'æ˜†æ˜å¸‚', 'æ˜†æ˜',
        'è¥¿è—è‡ªæ²»åŒº', 'è¥¿è—', 'æ‹‰è¨å¸‚', 'æ‹‰è¨',
        # --- è¥¿åŒ—åœ°åŒº ---
        'é™•è¥¿çœ', 'é™•è¥¿', 'è¥¿å®‰å¸‚', 'è¥¿å®‰',
        'ç”˜è‚ƒçœ', 'ç”˜è‚ƒ', 'å…°å·å¸‚', 'å…°å·',
        'é’æµ·çœ', 'é’æµ·', 'è¥¿å®å¸‚', 'è¥¿å®',
        'å®å¤å›æ—è‡ªæ²»åŒº', 'å®å¤', 'é“¶å·å¸‚', 'é“¶å·',
        'æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº', 'æ–°ç–†', 'ä¹Œé²æœ¨é½å¸‚', 'ä¹Œé²æœ¨é½',
        # --- æ¸¯æ¾³å° ---
        'é¦™æ¸¯', 'æ¾³é—¨', 'å°æ¹¾',
        # --- é€šç”¨åœ°ç†åç¼€ ---
        'çœ', 'å¸‚', 'å¿', 'åŒº', 'è‡ªæ²»å·', 'åœ°åŒº', 'é•‡', 'ä¹¡', 'æ‘',

        # ============================ è‹±æ–‡/æ‹¼éŸ³éƒ¨åˆ† (æ–°å¢) ============================
        # --- å›½å®¶ ---
        'china', 'p.r.c', 'prc',
        # --- ç›´è¾–å¸‚ ---
        'beijing shi', 'beijing', 'tianjin shi', 'tianjin', 'shanghai shi', 'shanghai', 'chongqing shi', 'chongqing',
        # --- ååŒ— ---
        'hebei province', 'hebei sheng', 'hebei', 'shijiazhuang', 'tangshan',
        'shanxi province', 'shanxi sheng', 'shanxi', 'taiyuan', 'datong',
        'inner mongolia autonomous region', 'inner mongolia', 'neimenggu', 'hohhot', 'baotou',
        # --- ä¸œåŒ— ---
        'liaoning province', 'liaoning sheng', 'liaoning', 'shenyang', 'dalian',
        'jilin province', 'jilin sheng', 'jilin', 'changchun',
        'heilongjiang province', 'heilongjiang sheng', 'heilongjiang', 'harbin', 'daqing',
        # --- åä¸œ ---
        'jiangsu province', 'jiangsu sheng', 'jiangsu', 'nanjing', 'suzhou', 'wuxi',
        'zhejiang province', 'zhejiang sheng', 'zhejiang', 'hangzhou', 'ningbo', 'wenzhou',
        'anhui province', 'anhui sheng', 'anhui', 'hefei',
        'fujian province', 'fujian sheng', 'fujian', 'fuzhou', 'xiamen', 'quanzhou',
        'jiangxi province', 'jiangxi sheng', 'jiangxi', 'nanchang',
        'shandong province', 'shandong sheng', 'shandong', 'jinan', 'qingdao', 'tsingtao', 'yantai',
        # --- åä¸­ ---
        'henan province', 'henan sheng', 'henan', 'zhengzhou', 'luoyang',
        'hubei province', 'hubei sheng', 'hubei', 'wuhan', 'yichang',
        'hunan province', 'hunan sheng', 'hunan', 'changsha',
        # --- åå— ---
        'guangdong province', 'guangdong sheng', 'guangdong', 'kwangtung',
        'guangzhou shi', 'guangzhou', 'canton',
        'shenzhen shi', 'shenzhen', 'dongguan', 'foshan', 'zhuhai',
        'guangxi zhuang autonomous region', 'guangxi', 'nanning', 'guilin',
        'hainan province', 'hainan sheng', 'hainan', 'haikou', 'sanya',
        # --- è¥¿å— ---
        'sichuan province', 'sichuan sheng', 'sichuan', 'szechuan', 'chengdu', 'mianyang',
        'guizhou province', 'guizhou sheng', 'guizhou', 'guiyang',
        'yunnan province', 'yunnan sheng', 'yunnan', 'kunming',
        'tibet autonomous region', 'tibet', 'xizang', 'lhasa',
        # --- è¥¿åŒ— ---
        'shaanxi province', 'shaanxi sheng', 'shaanxi', 'xi\'an', 'xian',
        'gansu province', 'gansu sheng', 'gansu', 'lanzhou',
        'qinghai province', 'qinghai sheng', 'qinghai', 'xining',
        'ningxia hui autonomous region', 'ningxia', 'yinchuan',
        'xinjiang uyghur autonomous region', 'xinjiang', 'urumqi',
        # --- æ¸¯æ¾³å° ---
        'hong kong', 'hongkong', 'hk', 'macau', 'macao', 'taiwan',
        # --- è‹±æ–‡é€šç”¨åœ°ç†åç¼€ ---
        'province', 'city', 'county', 'district', 'town', 'village', 'autonomous region', 'prefecture', 'region'
    ],

    # ==========================================================================
    # è¡Œä¸š/é€šç”¨è¯ (å·²å¢å¼ºè‹±æ–‡éƒ¨åˆ†)
    # ==========================================================================
    "INDUSTRIES": [
        # --- ä¸­æ–‡éƒ¨åˆ† ---
        'ä¿¡æ¯æŠ€æœ¯', 'ç½‘ç»œç§‘æŠ€', 'åœ¨çº¿','æ•°å­—ç§‘æŠ€', 'ç§‘æŠ€', 'æŠ€æœ¯', 'ä¿¡æ¯', 'ç½‘ç»œ', 'è½¯ä»¶', 'æ•°æ®', 'æ™ºèƒ½', 'ç”µå­', 'é€šä¿¡',
        'é‡‘è',  'æ§è‚¡', 'èµ„äº§', 'èµ„æœ¬', 'åŸºé‡‘', 'ç§Ÿèµ', 'å·¥ä¸š', 'åˆ¶é€ ', 'å®ä¸š',
        'å·¥ç¨‹', 'è£…å¤‡', 'èƒ½æº', 'åŒ–å·¥', 'ææ–™', 'ç”Ÿç‰©', 'åŒ»è¯', 'è´¸æ˜“', 'è¿›å‡ºå£', 'å•†è´¸', 'ç‰©æµ', 'ä¾›åº”é“¾',
        'å»ºç­‘', 'å»ºè®¾', 'æˆ¿äº§', 'åœ°äº§', 'ç½®ä¸š', 'ç‰©ä¸š', 'æ–‡åŒ–', 'ä¼ åª’', 'å¹¿å‘Š', 'å’¨è¯¢', 'æœåŠ¡', 'å‘å±•', 'ç®¡ç†',
        
        # --- è‹±æ–‡éƒ¨åˆ† ---
        'technology', 'science', 'engineering', 'information', 'electronics', 'energy', 'biology',
        'investment', 'finance', 'holdings', 'trading', 'international', 'tech', 'sci', 'info'
    ],
    
    # ==========================================================================
    # æ ‡ç‚¹/è¿æ¥ç¬¦ (ä¿æŒä¸å˜)
    # ==========================================================================
    "PUNCTUATION": [
        r'\(', r'\)', r'ï¼ˆ', r'ï¼‰', r'\[', r'\]', r'ã€', r'ã€‘',
        r'Â·', r'&', r',', r'/', r'\.', r'ã€'
    ]
}

def advanced_clean_name(name, stop_word_dict):
    """æ²¿ç”¨match_and_add_tags"""
    if not isinstance(name, str): return ""
    clean_name = name.strip().lower()
    punc_pattern = '|'.join(stop_word_dict["PUNCTUATION"])
    clean_name = re.sub(punc_pattern, ' ', clean_name)
    suffix_pattern = '|'.join(sorted(stop_word_dict["SUFFIXES"], key=len, reverse=True))
    clean_name = re.sub(r'(\s*\b(' + suffix_pattern + r')\b\s*)+$', '', clean_name)
    location_pattern = '|'.join(sorted(stop_word_dict["LOCATIONS"], key=len, reverse=True))
    clean_name = re.sub(f'^({location_pattern})', '', clean_name)
    industry_pattern = r'\b(' + '|'.join(sorted(stop_word_dict["INDUSTRIES"], key=len, reverse=True)) + r')\b'
    clean_name = re.sub(industry_pattern, '', clean_name)
    clean_name = re.sub(r'\s+', ' ', clean_name).strip()
    return clean_name

import re

def advanced_clean_name_v1(name, stop_word_dict):
    """
    1. ä¼˜å…ˆåˆ é™¤'-'åŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹ã€‚
    2. æ¥ç€åˆ é™¤æ‹¬å·åŠå…¶å†…éƒ¨çš„æ‰€æœ‰å†…å®¹ã€‚
    """
    if not isinstance(name, str): return ""
    
    clean_name = name.strip()
    # print("strip",clean_name)
    # ==================== æ ¸å¿ƒæ”¹è¿›ç‚¹ V3 ====================
    # æ­¥éª¤ 0.1: ä¼˜å…ˆç§»é™¤'-'åŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
    # ä½¿ç”¨ split è·å–ç¬¬ä¸€ä¸ªè¿å­—ç¬¦ä¹‹å‰çš„éƒ¨åˆ†ï¼Œå¹¶å»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼
    clean_name = clean_name.split('-', 1)[0].strip()
    # print("å­˜åœ¨çš„ç©ºæ ¼",clean_name)
    # æ­¥éª¤ 0.2: æ¥ç€ç§»é™¤æ‰€æœ‰æ‹¬å·ï¼ˆä¸­è‹±æ–‡ï¼‰åŠå…¶å†…éƒ¨çš„å†…å®¹
    clean_name = re.sub(r'[ï¼ˆ\(][^ï¼‰)]*[ï¼‰\)]', '', clean_name)
    # ======================================================
    # print("æ‹¬å·",clean_name)
    """
    ä½¿ç”¨åˆ†ç±»è¯åº“å’Œæ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ›´æ™ºèƒ½çš„åç§°æ¸…æ´—ã€‚
    """
    # 0. ç»Ÿä¸€å¤„ç†è‹±æ–‡å¤§å°å†™å’Œå‰åç©ºæ ¼
    clean_name = clean_name.strip().lower()
    # print("è‹±æ–‡å¤§å°",clean_name)
    # 1. ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·
    punc_pattern = '|'.join(stop_word_dict["PUNCTUATION"])
    clean_name = re.sub(punc_pattern, '', clean_name)
    # print("ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·",clean_name)
    # 2. ç§»é™¤å­—ç¬¦ä¸²æœ«å°¾çš„ç»„ç»‡åç¼€
    #    - '|' ç”¨äºè¿æ¥æ‰€æœ‰åç¼€ï¼Œå½¢æˆä¸€ä¸ªå¤§çš„"æˆ–"æ¨¡å¼
    #    - '$' è¡¨ç¤ºåŒ¹é…å­—ç¬¦ä¸²çš„ç»“å°¾
    # æ­¥éª¤ 3: ç§»é™¤å­—ç¬¦ä¸²æœ«å°¾çš„ç»„ç»‡åç¼€
    suffix_pattern = '|'.join(sorted(stop_word_dict["SUFFIXES"], key=len, reverse=True))
    clean_name = re.sub(r'(\s*(' + suffix_pattern + r')\s*)', '', clean_name)
    # print("SUFFIXES",clean_name)
    # æ­¥éª¤ 4: ç§»é™¤å­—ç¬¦ä¸²å¼€å¤´çš„åœ°ç†ä½ç½®
    # location_pattern = '|'.join(sorted(stop_word_dict["LOCATIONS"], key=len, reverse=True))
    # clean_name = re.sub(f'({location_pattern})', '', clean_name)
    # print("LOCATIONS",clean_name)
    # æ­¥éª¤ 5: ç§»é™¤ä¸­é—´çš„è¡Œä¸š/é€šç”¨è¯
    # industry_pattern =  '|'.join(sorted(stop_word_dict["INDUSTRIES"], key=len, reverse=True)) 
    # clean_name = re.sub(industry_pattern, '', clean_name)
    # print("INDUSTRIES",clean_name)


    # suffix_pattern = '|'.join(sorted(stop_word_dict["SUFFIXES"], key=len, reverse=True))
    # # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„ï¼ˆä¾‹å¦‚å…ˆåŒ¹é…'è‚¡ä»½æœ‰é™å…¬å¸'å†åŒ¹é…'å…¬å¸'ï¼‰
    # clean_name = re.sub(f'({suffix_pattern})$', '', clean_name)
    # # print("SUFFIXES",clean_name)
    # # 3. ç§»é™¤å­—ç¬¦ä¸²å¼€å¤´çš„åœ°ç†ä½ç½®
    # #    - '^' è¡¨ç¤ºåŒ¹é…å­—ç¬¦ä¸²çš„å¼€å¤´
    # location_pattern = '|'.join(sorted(stop_word_dict["LOCATIONS"], key=len, reverse=True))
    # clean_name = re.sub(f'^({location_pattern})', '', clean_name)
    # # print("LOCATIONS",clean_name)
    # # 4. ç§»é™¤ä¸­é—´çš„è¡Œä¸šé€šç”¨è¯ (è¿™ä¸€æ­¥è¦è°¨æ…ï¼Œå¯èƒ½ä¼šè¯¯ä¼¤)
    # #    è¿™é‡Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨æ›¿æ¢ï¼Œå› ä¸ºè¡Œä¸šè¯å¯èƒ½å‡ºç°åœ¨ä»»ä½•ä½ç½®
    # for word in sorted(stop_word_dict["INDUSTRIES"], key=len, reverse=True):
    #     clean_name = clean_name.replace(word, '')
    # print("INDUSTRIES",clean_name)
    # 5. æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    clean_name = clean_name.strip()
    # print("ç©ºæ ¼",clean_name)
    return clean_name

# advanced_clean_name_v1("è…¾è®¯ç§‘æŠ€ï¼ˆæ·±åœ³ï¼‰æ·±åœ³é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸", SMART_STOP_WORDS)
# exit(0)
# ==============================================================================
# æ­¥éª¤ 2: æ„å»ºé«˜æ•ˆçš„å†…å­˜æŸ¥æ‰¾æ•°æ®åº“
# ==============================================================================
def build_lookup_database(csv_file_path):
    """
    ä»CSVæ–‡ä»¶æ„å»ºä¸€ä¸ª 'æ ¸å¿ƒå…³é”®è¯ -> æ ‡å‡†å…¬å¸å' çš„æŸ¥æ‰¾å­—å…¸ã€‚
    è¿™æ˜¯æ•´ä¸ªæµç¨‹ä¸­æ€§èƒ½ä¼˜åŒ–çš„å…³é”®ã€‚
    """
    lookup_dict = {}
    print(f"æ­£åœ¨ä» '{csv_file_path}' æ„å»ºæŸ¥æ‰¾æ•°æ®åº“...")
    try:
        with open(csv_file_path, mode='r', encoding='utf-8-sig') as f: # 'utf-8-sig'èƒ½å¤„ç†å¸¦BOMçš„æ–‡ä»¶
            reader = csv.reader(f)
            # è·³è¿‡è¡¨å¤´
            header = next(reader)
            if header != ["å…¬å¸å", "åˆ«å"]:
                print(f"âš ï¸ è­¦å‘Š: CSVæ–‡ä»¶è¡¨å¤´ä¸æ˜¯é¢„æœŸçš„ ['å…¬å¸å', 'åˆ«å']ï¼Œè€Œæ˜¯ {header}ã€‚å°†ç»§ç»­å¤„ç†ã€‚")

            for row in reader:
                if len(row) < 2: continue # è·³è¿‡ä¸å®Œæ•´çš„è¡Œ
                
                standard_name = row[0].strip()
                aliases_str = row[1].strip()
                
                # è·å–æ‰€æœ‰å¾…å¤„ç†çš„åç§°ï¼ˆæ ‡å‡†å + æ‰€æœ‰åˆ«åï¼‰
                all_names = [standard_name] + [alias.strip() for alias in aliases_str.split(',') if alias.strip()]
                
                for name in all_names:
                    # æ¸…æ´—æ¯ä¸€ä¸ªåç§°ï¼Œå¾—åˆ°æ ¸å¿ƒå…³é”®è¯
                    core_keyword = advanced_clean_name_v1(name, SMART_STOP_WORDS)
                    
                    # å¦‚æœæ ¸å¿ƒå…³é”®è¯éç©ºï¼Œä¸”å°šæœªåœ¨å­—å…¸ä¸­ï¼ˆé¿å…ä½ä¼˜å…ˆçº§åˆ«åè¦†ç›–é«˜ä¼˜å…ˆçº§ï¼‰
                    if core_keyword and core_keyword not in lookup_dict:
                        lookup_dict[core_keyword] = standard_name
                        
    except FileNotFoundError:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ•°æ®åº“æ–‡ä»¶ '{csv_file_path}' æœªæ‰¾åˆ°ï¼è¯·æ£€æŸ¥æ–‡ä»¶åå’Œè·¯å¾„ã€‚")
        return None
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None
    
    print(f"âœ… æŸ¥æ‰¾æ•°æ®åº“æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(lookup_dict)} ä¸ªå”¯ä¸€çš„å…³é”®è¯ã€‚")
    return lookup_dict

# ==============================================================================
# æ­¥éª¤ 3: ä¸»é€»è¾‘ - éå†ã€åŒ¹é…ã€æ›¿æ¢
# ==============================================================================
def standardize_rankings(input_json_path, db_lookup, output_json_path):
    """
    éå†æ¦œå•JSONï¼Œä½¿ç”¨æŸ¥æ‰¾å­—å…¸è¿›è¡Œåç§°æ ‡å‡†åŒ–ã€‚
    """
    print(f"\næ­£åœ¨å¤„ç†æ¦œå•æ–‡ä»¶: '{input_json_path}'")
    try:
        with open(input_json_path, 'r', encoding='utf-8') as f:
            original_rankings = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ¦œå•æ–‡ä»¶ '{input_json_path}' æœªæ‰¾åˆ°ï¼")
        return
    except json.JSONDecodeError:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ¦œå•æ–‡ä»¶ '{input_json_path}' ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚")
        return

    normalized_rankings = {}
    success_count = 0
    failure_count = 0

    # éå†æ¯ä¸ªæ¦œå•
    for ranking_name, company_list in original_rankings.items():
        new_company_list = []
        # éå†æ¦œå•ä¸‹çš„æ¯ä¸ªå…¬å¸
        for company_name in company_list:
            # æ¸…æ´—æ¦œå•ä¸­çš„å…¬å¸åï¼Œå¾—åˆ°æŸ¥è¯¢å…³é”®è¯
            query_core = advanced_clean_name_v1(company_name, SMART_STOP_WORDS)
            
            is_match = False
            # è¿›è¡Œé«˜æ•ˆçš„å­—å…¸æŸ¥æ‰¾ï¼Œè€Œä¸æ˜¯å¾ªç¯æ¯”è¾ƒ
            if query_core and query_core in db_lookup:
                standard_name = db_lookup[query_core]
                new_company_list.append(standard_name)
                success_count += 1
                is_match = True
            
            if not is_match:
                # å¦‚æœåŒ¹é…å¤±è´¥ï¼Œä¿ç•™åŸåå¹¶è¾“å‡ºæç¤º
                new_company_list.append(company_name)
                print(f"  - åŒ¹é…å¤±è´¥: [æ¦œå•: {ranking_name}] - å…¬å¸: {company_name} - æ¸…æ´—ï¼š{query_core}")
                failure_count += 1
        
        # å°†å¤„ç†åçš„å…¬å¸åˆ—è¡¨å­˜å…¥æ–°çš„å­—å…¸
        # ä½¿ç”¨ set å»é‡ï¼Œä¿ç•™åŸå§‹é¡ºåºç›¸å¯¹å›°éš¾ï¼Œè¿™é‡Œç›´æ¥å»é‡å¹¶æ’åº
        normalized_rankings[ranking_name] = sorted(list(set(new_company_list)))

    print("\næ ‡å‡†åŒ–å¤„ç†å®Œæˆï¼")
    
    # ä¿å­˜ç»“æœåˆ°æ–°çš„JSONæ–‡ä»¶
    try:
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(normalized_rankings, f, ensure_ascii=False, indent=2)

        print("\n--- ä»»åŠ¡æ€»ç»“ ---")
        print(f"âœ”ï¸  æˆåŠŸåŒ¹é…å¹¶æ ‡å‡†åŒ–çš„è®°å½•æ•°: {success_count}")
        print(f"âŒ  åŒ¹é…å¤±è´¥çš„è®°å½•æ•°: {failure_count}")
        print(f"ğŸ‰  æ ‡å‡†åŒ–åçš„æ•°æ®å·²æˆåŠŸå†™å…¥åˆ°: '{output_json_path}'")
        
    except Exception as e:
        print(f"ğŸ”¥  ä¸¥é‡é”™è¯¯: æ— æ³•å°†ç»“æœå†™å…¥åˆ° '{output_json_path}': {e}")


if __name__ == '__main__':
    # --- é…ç½®åŒº ---
    # è¾“å…¥æ–‡ä»¶
    aggregated_json_file = 'aggregated_rankings.json'
    database_csv_file = './res/å…¬å¸å_åˆ«å.csv'
    
    # è¾“å‡ºæ–‡ä»¶
    normalized_json_file = 'normalized_rankings.json'
    
    # --- è¿è¡Œä¸»æµç¨‹ ---
    # 1. æ„å»ºå†…å­˜æ•°æ®åº“
    lookup_db = build_lookup_database(database_csv_file)
    
    # 2. å¦‚æœæ•°æ®åº“æ„å»ºæˆåŠŸï¼Œåˆ™æ‰§è¡Œæ ‡å‡†åŒ–
    if lookup_db:
        standardize_rankings(aggregated_json_file, lookup_db, normalized_json_file)