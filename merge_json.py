import os
import json
from collections import defaultdict

def aggregate_by_ranking(root_folders, output_file):
    """
    éå†æŒ‡å®šçš„æ ¹æ–‡ä»¶å¤¹ï¼ŒæŒ‰æ¦œå•åç§°èšåˆæ‰€æœ‰ company_list.json æ–‡ä»¶ã€‚
    æœ€ç»ˆç”Ÿæˆä¸€ä¸ªä»¥æ¦œå•åä¸ºé”®ï¼Œå…¬å¸åˆ—è¡¨ä¸ºå€¼çš„å­—å…¸ã€‚
    """
    # ä½¿ç”¨ defaultdict(list) å¯ä»¥æå¤§åœ°ç®€åŒ–ä»£ç 
    # å½“è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„é”®æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ä½œä¸ºé»˜è®¤å€¼
    aggregated_data = defaultdict(list)
    
    processed_files_count = 0
    skipped_empty_count = 0
    skipped_missing_count = 0

    print("å¼€å§‹æŒ‰æ¦œå•èšåˆä»»åŠ¡...")
    print(f"æ‰«ææ–‡ä»¶å¤¹: {', '.join(root_folders)}")
    print("-" * 30)

    # 1. éå†æ¯ä¸€ä¸ªä¸»æ–‡ä»¶å¤¹
    for folder in root_folders:
        if not os.path.isdir(folder):
            print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤¹ '{folder}' ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
            continue

        print(f"\næ­£åœ¨å¤„ç†ä¸»æ–‡ä»¶å¤¹: '{folder}'")
        
        # 2. éå†ä¸»æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¦œå•å­æ–‡ä»¶å¤¹
        for subfolder_name in os.listdir(folder):
            subfolder_path = os.path.join(folder, subfolder_name)

            if os.path.isdir(subfolder_path):
                json_path = os.path.join(subfolder_path, 'company_list.json')

                # 3. æ£€æŸ¥ company_list.json æ˜¯å¦å­˜åœ¨
                if os.path.exists(json_path):
                    try:
                        with open(json_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)

                        # ==================== æ ¸å¿ƒé€»è¾‘ä¿®æ”¹å¤„ ====================
                        
                        found_data_in_file = False
                        
                        # åªå¤„ç†å­—å…¸æ ¼å¼: {"æ¦œå•å": [...]}
                        if isinstance(data, dict) and data:
                            for ranking_name, company_list in data.items():
                                # ç¡®ä¿æ¦œå•åä¸ä¸ºç©ºï¼Œä¸”å…¬å¸åˆ—è¡¨æ˜¯æœ‰æ•ˆçš„åˆ—è¡¨
                                if ranking_name and isinstance(company_list, list) and company_list:
                                    # ä½¿ç”¨ defaultdict çš„é­”åŠ›ï¼
                                    # å¦‚æœ ranking_name ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»º aggregated_data[ranking_name] = []
                                    # ç„¶å extend æ“ä½œ
                                    aggregated_data[ranking_name].extend(company_list)
                                    
                                    print(f"  âœ… å·²èšåˆ: {json_path} (æ¦œå•'{ranking_name}', {len(company_list)}æ¡è®°å½•)")
                                    found_data_in_file = True
                        
                        # ======================================================
                        
                        if found_data_in_file:
                            processed_files_count += 1
                        else:
                            skipped_empty_count += 1
                            print(f"  ğŸŸ¡ å·²è·³è¿‡ (ç©ºå†…å®¹æˆ–éå­—å…¸æ ¼å¼): {json_path}")

                    except json.JSONDecodeError:
                        print(f"  âŒ é”™è¯¯: æ–‡ä»¶ '{json_path}' ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå·²è·³è¿‡ã€‚")
                    except Exception as e:
                        print(f"  âŒ é”™è¯¯: è¯»å–æ–‡ä»¶ '{json_path}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                else:
                    skipped_missing_count += 1
    
    print("-" * 30)
    print("\nèšåˆä»»åŠ¡å®Œæˆï¼")

    # ä¸ºäº†æ›´å¥½çš„å¯è¯»æ€§ï¼Œå¯¹æœ€ç»ˆç»“æœçš„é”®ï¼ˆæ¦œå•åï¼‰è¿›è¡Œæ’åº
    sorted_aggregated_data = dict(sorted(aggregated_data.items()))
    
    # å¯é€‰ï¼šå¯¹æ¯ä¸ªæ¦œå•ä¸‹çš„å…¬å¸åˆ—è¡¨å»é‡å¹¶æ’åº
    for ranking_name in sorted_aggregated_data:
        # ä½¿ç”¨ set å»é‡ï¼Œç„¶åè½¬å› list å¹¶æ’åº
        unique_companies = sorted(list(set(sorted_aggregated_data[ranking_name])))
        sorted_aggregated_data[ranking_name] = unique_companies

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(sorted_aggregated_data, f, ensure_ascii=False, indent=2)
        
        print("\n--- ä»»åŠ¡æ€»ç»“ ---")
        print(f"âœ”ï¸  æˆåŠŸå¤„ç†æ–‡ä»¶æ•°é‡: {processed_files_count}")
        print(f"âœ”ï¸  èšåˆåæ€»æ¦œå•æ•°: {len(sorted_aggregated_data)}")
        total_companies = sum(len(v) for v in sorted_aggregated_data.values())
        print(f"âœ”ï¸  æ‰€æœ‰æ¦œå•ä¸‹å…¬å¸æ€»æ•° (å»é‡å): {total_companies}")
        print(f"ğŸŸ¡  å› å†…å®¹ä¸ºç©º/æ ¼å¼ä¸ç¬¦è€Œè·³è¿‡çš„æ–‡ä»¶æ•°: {skipped_empty_count}")
        print(f"ğŸ‰  æ‰€æœ‰æ•°æ®å·²æˆåŠŸå†™å…¥åˆ°: '{output_file}'")

    except Exception as e:
        print(f"ğŸ”¥  ä¸¥é‡é”™è¯¯: æ— æ³•å°†æ•°æ®å†™å…¥åˆ° '{output_file}': {e}")


if __name__ == '__main__':
    # --- é…ç½®åŒº ---
    folders_to_scan = [
        'web_rankings_pdf_output',
        'web_rankings_pdf_output2',
        'web_rankings_pdf_output3'
    ]
    # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åä»¥åæ˜ æ–°çš„æ•°æ®ç»“æ„
    final_output_file = 'aggregated_rankings.json'

    # --- è¿è¡Œä¸»å‡½æ•° ---
    aggregate_by_ranking(folders_to_scan, final_output_file)